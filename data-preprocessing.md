# âœ… ONE-PAGE DATA PREPROCESSING PIPELINE  
(Fresher + Company-Level Interview & Project Ready)

---

## ğŸš€ 0. Import & Load Data

```python
import pandas as pd
import numpy as np

df = pd.read_csv("data.csv")
````

---

## ğŸš€ 1. Inspect Data (ALWAYS FIRST)

```python
df.head()
df.tail()
df.info()
df.describe()
df.shape
df.dtypes
```

### ğŸ¯ Purpose:

* Understand columns
* Detect missing values
* Identify numerical vs categorical
* Check data types & scale

---

## ğŸš€ 2. Handle Missing Values

### ğŸ” Identify

```python
df.isnull().sum()
```

### âœ… Handle in Pandas

```python
df.dropna()                      # Remove rows
df.fillna(0)                     # Constant fill
df.fillna(df.mean())             # Numerical
df.fillna(df.mode().iloc[0])     # Categorical
```

### âœ… ML-Friendly (BEST PRACTICE)

```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="mean")
X = imputer.fit_transform(X)
```

---

## ğŸš€ 3. Handle Categorical Data

### âœ… Label Encoding (Binary / Ordinal)

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["gender"] = le.fit_transform(df["gender"])
```

### âœ… One-Hot Encoding (Nominal)

```python
pd.get_dummies(df, drop_first=True)
```

### âœ… ML Pipeline Encoding (BEST)

```python
from sklearn.preprocessing import OneHotEncoder
```

---

## ğŸš€ 4. Handle Outliers

### âœ… Using IQR Method

```python
Q1 = df["age"].quantile(0.25)
Q3 = df["age"].quantile(0.75)
IQR = Q3 - Q1

filtered_df = df[
    (df["age"] >= Q1 - 1.5 * IQR) &
    (df["age"] <= Q3 + 1.5 * IQR)
]
```

---

## ğŸš€ 5. Feature Scaling

### âœ… Standardization (Most Used)

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### âœ… Normalization (0â€“1 Range)

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

### â— When to Scale?

âœ… Logistic Regression
âœ… SVM
âœ… KNN
âœ… Neural Networks

âŒ Not needed for:

* Decision Tree
* Random Forest
* XGBoost

---

## ğŸš€ 6. Feature Engineering

### âœ… Create New Features

```python
df["income_yearly"] = df["income_monthly"] * 12

df["age_group"] = df["age"].apply(
    lambda x: "Adult" if x >= 18 else "Child"
)
```

### âœ… Binning

```python
pd.cut(df["age"], bins=[0,18,40,60,100])
```

### âœ… Date Features

```python
df["year"] = pd.to_datetime(df["date"]).dt.year
df["month"] = pd.to_datetime(df["date"]).dt.month
df["weekday"] = pd.to_datetime(df["date"]).dt.weekday
```

---

## ğŸš€ 7. Remove Duplicates

```python
df.drop_duplicates(inplace=True)
```

---

## ğŸš€ 8. Remove Irrelevant Columns

```python
df.drop(["id", "name"], axis=1, inplace=True)
```

---

## ğŸš€ 9. Train-Test Split (MANDATORY)

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

---

## ğŸš€ 10. Build Preprocessing Pipeline (ğŸ”¥ BEST PRACTICE ğŸ”¥)

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler()),
    ("model", LogisticRegression())
])

pipeline.fit(X_train, y_train)
```

âœ… Prevents **data leakage**
âœ… Ensures **clean ML workflow**

---

## ğŸš€ 11. Feature Selection

### âœ… Correlation

```python
df.corr()
```

### âœ… SelectKBest

```python
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X, y)
```

### âœ… Tree-Based Feature Importance

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X, y)
model.feature_importances_
```

---

## ğŸš€ 12. Final Model Training

```python
model.fit(X_train, y_train)
```

---

## ğŸš€ 13. Model Evaluation

### âœ… Classification

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
```

### âœ… Regression

```python
from sklearn.metrics import mean_squared_error, r2_score
```

---

# ğŸ¯ FULL DATA PREPROCESSING FLOW (REAL-WORLD)

1. Load Data
2. Understand Data
3. Handle Missing Values
4. Handle Categorical Data
5. Detect & Treat Outliers
6. Feature Scaling
7. Feature Engineering
8. Remove Duplicates
9. Remove Irrelevant Columns
10. Train-Test Split
11. Build Pipeline
12. Feature Selection
13. Model Training
14. Evaluation

âœ… This is **production-level ML workflow**

# ğŸš€ TOP 25 DATA PREPROCESSING INTERVIEW QUESTIONS (WITH ANSWERS)

(Company-Level Pack)

---

## 1ï¸âƒ£ What is data preprocessing?

**Answer:**
Cleaning & transforming raw data into ML-ready format.

Includes:

* Missing values
* Encoding
* Scaling
* Feature engineering
* Feature selection

---

## 2ï¸âƒ£ Why is data preprocessing important?

**Answer:**
Because raw data contains:

* Missing values
* Noise
* Duplicates
* Outliers

Improves:
âœ… Accuracy
âœ… Stability
âœ… Training speed

---

## 3ï¸âƒ£ How do you handle missing values?

```python
df.dropna()
df.fillna(0)
df.fillna(df["col"].mean())
```

âœ… Best ML method â†’ `SimpleImputer`

---

## 4ï¸âƒ£ dropna() vs fillna()?

* `dropna()` â†’ removes rows
* `fillna()` â†’ fills values

---

## 5ï¸âƒ£ How do you detect outliers?

âœ… IQR
âœ… Z-score
âœ… Boxplot
âœ… Isolation Forest

---

## 6ï¸âƒ£ How do you treat outliers?

* Remove
* Cap (winsorize)
* Transform (log/sqrt)
* Use robust models (Random Forest, XGBoost)

---

## 7ï¸âƒ£ What is feature scaling?

**Answer:**
Transforms features to same scale so no feature dominates.

---

## 8ï¸âƒ£ StandardScaler vs MinMaxScaler?

| StandardScaler    | MinMaxScaler    |
| ----------------- | --------------- |
| Mean = 0, Std = 1 | Range 0â€“1       |
| LR, SVM, KNN      | Neural Networks |

---

## 9ï¸âƒ£ Do tree models need scaling?

âŒ No. Trees split on thresholds, not distance.

---

## ğŸ”Ÿ What is categorical encoding?

Converting text â†’ numbers using:

* Label Encoding
* One-Hot Encoding
* Target Encoding (advanced)

---

## 11ï¸âƒ£ LabelEncoder vs OneHotEncoder?

* LabelEncoder â†’ 0,1,2â€¦
* OneHotEncoder â†’ Binary columns
  âœ… Use OneHot if category is **non-ordinal**

---

## 12ï¸âƒ£ What is feature engineering?

Creating new useful features.

Examples:

* age â†’ age_group
* date â†’ year/month
* income â†’ yearly income

---

## 13ï¸âƒ£ What is feature selection?

Choosing most important features to:
âœ… Reduce overfitting
âœ… Improve accuracy

Methods:

* Correlation
* SelectKBest
* Tree importance

---

## 14ï¸âƒ£ What is a data pipeline?

Pipeline = preprocessing + model in one flow.
âœ… Prevents data leakage
âœ… Keeps transformations consistent

---

## 15ï¸âƒ£ What is data leakage?

Test data influencing training.

Example:
âŒ Scaling before train-test split
âœ… Fix â†’ Use pipeline

---

## 16ï¸âƒ£ Why is train-test split needed?

To test model on **unseen data** and avoid overfitting.

---

## 17ï¸âƒ£ How do you check imbalanced data?

```python
df["target"].value_counts()
```

---

## 18ï¸âƒ£ How do you handle imbalanced data?

* SMOTE
* Undersampling
* Class weights
* XGBoost / CatBoost

---

## 19ï¸âƒ£ What is normalization?

Scaling values between **0 and 1**.
Used mainly for **Neural Networks**.

---

## 20ï¸âƒ£ How to remove duplicates?

```python
df.drop_duplicates(inplace=True)
```

---

# ğŸ”¥ SUPER BONUS (ADVANCED COMPANY QUESTIONS)

---

## 21ï¸âƒ£ High-cardinality encoding methods?

âœ… Target Encoding
âœ… Hash Encoding
âœ… CatBoost Encoder

---

## 22ï¸âƒ£ Why scaling not needed in Random Forest?

Because trees split by **value comparison**, not distance.

---

## 23ï¸âƒ£ Normalization vs Standardization?

* Normalization â†’ 0â€“1 range
* Standardization â†’ Mean 0, Std 1

---

## 24ï¸âƒ£ When to use MinMaxScaler?

âœ… Neural Networks
âœ… CNN / RNN
âœ… Bounded activation functions

---

## 25ï¸âƒ£ What is One-Hot Encoding Trap?

Dummy variable trap â†’ **Multicollinearity**
âœ… Fix â†’ `drop_first=True`

---
